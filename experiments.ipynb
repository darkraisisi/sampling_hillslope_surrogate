{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulation import minimal_model as mm\n",
    "from surrogate import neural_network\n",
    "\n",
    "from sampling import grid, random, lhs\n",
    "from visualise import stream, surface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EGO test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import norm  # Add this import\n",
    "\n",
    "class EGO:\n",
    "    def __init__(self, func, bounds, initial_points, noise=0.1):\n",
    "        self.func = func\n",
    "        self.bounds = bounds\n",
    "        self.noise = noise\n",
    "        self.X = np.array(initial_points)\n",
    "        self.y = np.array([self.func(x) for x in initial_points]).reshape(-1, 1)\n",
    "        self.scaler = StandardScaler().fit(self.X)\n",
    "        self.gpr = GaussianProcessRegressor(kernel=RBF(), alpha=noise**2)\n",
    "\n",
    "    def surrogate(self, X):\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        return self.gpr.predict(X_scaled, return_std=True)\n",
    "\n",
    "    def acquisition(self, X):\n",
    "        X = np.atleast_2d(X)  # Ensure X is 2D\n",
    "        mu, sigma = self.surrogate(X)\n",
    "        mu_opt = np.min(self.y)\n",
    "        z = (mu_opt - mu) / sigma\n",
    "        ei = (mu_opt - mu) * norm.cdf(z) + sigma * norm.pdf(z)\n",
    "        return -ei\n",
    "\n",
    "    def optimize_acquisition(self):\n",
    "        bounds = [(low, high) for low, high in self.bounds]\n",
    "        result = minimize(self.acquisition, np.random.uniform(*zip(*self.bounds)), bounds=bounds)\n",
    "        return result.x\n",
    "\n",
    "    def suggest_next_point(self, n_iter=1):\n",
    "        for _ in range(n_iter):\n",
    "            self.gpr.fit(self.scaler.transform(self.X), self.y)\n",
    "            next_point = self.optimize_acquisition()\n",
    "            self.X = np.vstack((self.X, next_point.reshape(1, -1)))\n",
    "            self.y = np.vstack((self.y, self.func(next_point).reshape(1, -1)))\n",
    "\n",
    "        return self.X[-1]\n",
    "\n",
    "# Example usage\n",
    "def test_function(x):\n",
    "    x = x[0]\n",
    "    return (x-2)*(x-5)*(x-8) + np.random.normal(0, 0.1)\n",
    "\n",
    "ego_optimizer = EGO(func=test_function, bounds=[(0, 10)], initial_points=[[5]])\n",
    "next_point = ego_optimizer.suggest_next_point()\n",
    "print(\"Next point to evaluate:\", next_point)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = 20\n",
    "ego_optimizer = EGO(func=test_function, bounds=[(0, 10)], initial_points=[[5]])\n",
    "fig, ax = plt.subplots(it//2,2, figsize=(10,20))\n",
    "for i in range(it):\n",
    "    row_id = i//2\n",
    "    col_id = i%2\n",
    "    ax[row_id, col_id].plot(ego_optimizer.X,ego_optimizer.y, \"o\")\n",
    "    ego_optimizer.suggest_next_point()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LHS test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import copy\n",
    "# import math\n",
    "\n",
    "# def latin_hypercube_sampling(bounds, n_samples):\n",
    "#     \"\"\"\n",
    "#     Generate Latin Hypercube Samples.\n",
    "\n",
    "#     Parameters:\n",
    "#         bounds (list of tuples): List of tuples where each tuple represents the lower and upper bounds of the variables.\n",
    "#         n_samples (int): Number of samples to generate.\n",
    "\n",
    "#     Returns:\n",
    "#         np.array: Array of shape (n_samples, n_variables) containing the Latin Hypercube samples.\n",
    "#     \"\"\"\n",
    "#     n_variables = len(bounds)\n",
    "#     samples = np.zeros((n_samples, n_variables))\n",
    "\n",
    "#     # Generate quantiles for each variable\n",
    "#     quantiles = np.linspace(0, 1, n_samples + 1)\n",
    "\n",
    "#     # Generate random permutation of indices for each variable\n",
    "#     indices = [list(range(n_samples)) for _ in range(n_variables)]\n",
    "#     for i in range(n_variables):\n",
    "#         np.random.shuffle(indices[i])\n",
    "\n",
    "#     # Assign values from quantiles to the samples\n",
    "#     for i in range(n_variables):\n",
    "#         for j in range(n_samples):\n",
    "#             samples[j, i] = np.random.uniform(quantiles[indices[i][j]], quantiles[indices[i][j] + 1]) * (bounds[i][1] - bounds[i][0]) + bounds[i][0]\n",
    "\n",
    "#     return samples\n",
    "\n",
    "# def calculate_objective(samples):\n",
    "#     \"\"\"\n",
    "#     Calculate the objective function.\n",
    "\n",
    "#     Parameters:\n",
    "#         samples (np.array): Array of shape (n_samples, n_variables) containing the samples.\n",
    "\n",
    "#     Returns:\n",
    "#         float: Value of the objective function.\n",
    "#     \"\"\"\n",
    "#     # Example objective function: sum of squares of differences between samples\n",
    "#     return np.sum(np.sum(np.diff(samples, axis=0) ** 2))\n",
    "\n",
    "# def perturb_sample(sample, bounds, temperature):\n",
    "#     \"\"\"\n",
    "#     Perturb a sample using simulated annealing.\n",
    "\n",
    "#     Parameters:\n",
    "#         sample (np.array): Array representing the sample to be perturbed.\n",
    "#         bounds (np.array): Array of shape (n_variables, 2) representing the lower and upper bounds of the variables.\n",
    "#         temperature (float): Current temperature in simulated annealing.\n",
    "\n",
    "#     Returns:\n",
    "#         np.array: Perturbed sample.\n",
    "#     \"\"\"\n",
    "#     perturbed_sample = np.copy(sample)\n",
    "#     for i in range(sample.shape[1]):  # Iterate over the number of variables (columns)\n",
    "#         lower_bound, upper_bound = bounds[i]\n",
    "#         for j in range(sample.shape[0]):  # Iterate over the number of samples (rows)\n",
    "#             perturbed_sample[j, i] = np.minimum(upper_bound, np.maximum(lower_bound, sample[j, i] + np.random.uniform(-temperature, temperature)))\n",
    "#     return perturbed_sample\n",
    "\n",
    "# def simulated_annealing_lhs(bounds, n_samples, max_iterations, initial_temperature, cooling_rate):\n",
    "#     \"\"\"\n",
    "#     Perform Latin Hypercube Sampling (LHS) with Simulated Annealing.\n",
    "\n",
    "#     Parameters:\n",
    "#         bounds (list of tuples): List of tuples where each tuple represents the lower and upper bounds of the variables.\n",
    "#         n_samples (int): Number of samples to generate.\n",
    "#         max_iterations (int): Maximum number of iterations for simulated annealing.\n",
    "#         initial_temperature (float): Initial temperature for simulated annealing.\n",
    "#         cooling_rate (float): Cooling rate for simulated annealing.\n",
    "\n",
    "#     Returns:\n",
    "#         np.array: Array of shape (n_samples, n_variables) containing the Latin Hypercube samples.\n",
    "#     \"\"\"\n",
    "#     current_samples = latin_hypercube_sampling(bounds, n_samples)\n",
    "#     current_objective = calculate_objective(current_samples)\n",
    "#     best_samples = copy.deepcopy(current_samples)\n",
    "#     best_objective = current_objective\n",
    "\n",
    "#     temperature = initial_temperature\n",
    "#     iteration = 0\n",
    "\n",
    "#     while iteration < max_iterations and temperature > 0:\n",
    "#         new_samples = perturb_sample(current_samples, bounds, temperature)\n",
    "#         new_objective = calculate_objective(new_samples)\n",
    "#         delta_objective = new_objective - current_objective\n",
    "\n",
    "#         if delta_objective < 0 or random.random() < math.exp(-delta_objective / temperature):\n",
    "#             current_samples = new_samples\n",
    "#             current_objective = new_objective\n",
    "\n",
    "#         if current_objective < best_objective:\n",
    "#             best_samples = copy.deepcopy(current_samples)\n",
    "#             best_objective = current_objective\n",
    "\n",
    "#         temperature *= cooling_rate\n",
    "#         iteration += 1\n",
    "\n",
    "#     return best_samples\n",
    "\n",
    "# # Example usage:\n",
    "# bounds = np.array([(0, 10), (-5, 5)])  # Example bounds for three variables\n",
    "# n_samples = 10  # Number of samples\n",
    "# max_iterations = 1000  # Maximum number of iterations for simulated annealing\n",
    "# initial_temperature = 100  # Initial temperature for simulated annealing\n",
    "# cooling_rate = 0.90  # Cooling rate for simulated annealing\n",
    "\n",
    "# samples = simulated_annealing_lhs(bounds, n_samples, max_iterations, initial_temperature, cooling_rate)\n",
    "# # print(samples)\n",
    "# plt.plot(samples[:,0], samples[:,1], \"o\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_adaptive_lhs(current_samples, bounds, variance_grid, max_iterations, initial_temperature, cooling_rate):\n",
    "    \"\"\"\n",
    "    Perform Latin Hypercube Sampling (LHS) with Sequential Adaptive Method.\n",
    "\n",
    "    Parameters:\n",
    "        current_samples (np.array): Array of shape (n_samples, n_variables) containing the current samples.\n",
    "        bounds (np.array): Array of shape (n_variables, 2) representing the lower and upper bounds of the variables.\n",
    "        variance_grid (np.array): Array of shape (n_samples, n_variables) containing the variance for each variable-sample combination.\n",
    "        max_iterations (int): Maximum number of iterations for sequential adaptive method.\n",
    "        initial_temperature (float): Initial temperature for simulated annealing.\n",
    "        cooling_rate (float): Cooling rate for simulated annealing.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Array of shape (n_samples, n_variables) containing the Latin Hypercube samples.\n",
    "    \"\"\"\n",
    "    current_objective = calculate_objective(current_samples)\n",
    "    best_samples = copy.deepcopy(current_samples)\n",
    "    best_objective = current_objective\n",
    "\n",
    "    temperature = initial_temperature\n",
    "    iteration = 0\n",
    "\n",
    "    while iteration < max_iterations and temperature > 0:\n",
    "        new_samples = perturb_sample_adaptive(current_samples, bounds, variance_grid, temperature)\n",
    "        new_objective = calculate_objective(new_samples)\n",
    "        delta_objective = new_objective - current_objective\n",
    "\n",
    "        if delta_objective < 0 or random.random() < math.exp(-delta_objective / temperature):\n",
    "            current_samples = new_samples\n",
    "            current_objective = new_objective\n",
    "\n",
    "        if current_objective < best_objective:\n",
    "            best_samples = copy.deepcopy(current_samples)\n",
    "            best_objective = current_objective\n",
    "\n",
    "        temperature *= cooling_rate\n",
    "        iteration += 1\n",
    "\n",
    "    return best_samples\n",
    "\n",
    "def perturb_sample_adaptive(sample, bounds, variance_grid, temperature):\n",
    "    \"\"\"\n",
    "    Perturb a sample using adaptive perturbation.\n",
    "\n",
    "    Parameters:\n",
    "        sample (np.array): Array representing the sample to be perturbed.\n",
    "        bounds (np.array): Array of shape (n_variables, 2) representing the lower and upper bounds of the variables.\n",
    "        variance_grid (np.array): Array of shape (n_samples, n_variables) containing the variance for each variable-sample combination.\n",
    "        temperature (float): Current temperature in simulated annealing.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Perturbed sample.\n",
    "    \"\"\"\n",
    "    perturbed_sample = np.copy(sample)\n",
    "    n_samples, n_variables = sample.shape\n",
    "    for i in range(n_variables):  # Iterate over the number of variables (columns)\n",
    "        lower_bound, upper_bound = bounds[i]\n",
    "        for j in range(n_samples):  # Iterate over the number of samples (rows)\n",
    "            variance = variance_grid[j, i]  # Variance for the current variable-sample combination\n",
    "            perturbed_sample[j, i] = np.minimum(upper_bound, np.maximum(lower_bound, sample[j, i] + np.random.normal(0, variance) * temperature))\n",
    "    return perturbed_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance = np.zeros((100,100))\n",
    "variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = sequential_adaptive_lhs(samples, bounds, variance, max_iterations, initial_temperature, cooling_rate)\n",
    "# # print(samples)\n",
    "# plt.plot(samples[:,0], samples[:,1], \"o\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_samples = latin_hypercube_sampling(bounds, 100)\n",
    "# current_samples\n",
    "# plt.plot(current_samples[:,0], current_samples[:,1], \"o\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_objective = calculate_objective(current_samples)\n",
    "# current_objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 400\n",
    "grid_n = int(np.sqrt(n_samples))\n",
    "n_samples = grid_n * grid_n\n",
    "# Minimal model testing\n",
    "g = 1.7\n",
    "B_lim, D_lim = 2.9, 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LHS search\n",
    "LHS = lhs.LatinHyperCubeStack()\n",
    "D_grid, B_grid = LHS.sample_stack([(0, D_lim), (0, B_lim)], n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import qmc\n",
    "sampler = qmc.LatinHypercube(d=2)\n",
    "sample = sampler.random(n=n_samples)\n",
    "sample_scaled = qmc.scale(sample, [0, 0], [B_lim, D_lim])\n",
    "plt.plot(sample_scaled[:,0], sample_scaled[:,1], \"o\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D_grid, B_grid = np.meshgrid(sample_scaled[:,0], sample_scaled[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RS = random.RandomStack()\n",
    "D_grid, B_grid = RS.sample_stack([(0, D_lim), (0, B_lim)], grid_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Grid search\n",
    "EG = grid.EqualStack()\n",
    "D_grid, B_grid = EG.sample_stack([(0, D_lim), (0, B_lim)], grid_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run miminal model\n",
    "dB_dt, dD_dt = mm.step(B_grid, D_grid, g, warm_up=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(B_grid, D_grid, \"o\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stream.show(D_grid, B_grid, dD_dt, dB_dt, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# surface.show(D_grid, B_grid, dD_dt, dB_dt, D_lim, B_lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dB_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scipy.signal.convolve2d(dB_dt, np.ones((2,2)), mode=\"same\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.ones((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_grid, D_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col_i = np.argmax(np.sum(dB_dt, axis=0))\n",
    "# row_i = np.argmax(np.sum(dB_dt, axis=1))\n",
    "# print(dB_dt, col_i, row_i)\n",
    "# B_grid[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_B, x = np.gradient(dB_dt)\n",
    "grad_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(grad_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(np.abs(grad_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_grid.shape, D_grid.shape, n_samples, dB_dt.shape, dD_dt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ['B', 'D', 'g', 'dB_dt', 'dD_dt']\n",
    "X = np.column_stack([B_grid.flatten(), D_grid.flatten(), np.repeat(g, (n_samples))])\n",
    "y = np.column_stack([dB_dt.flatten(), dD_dt.flatten()])\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_history(uid):\n",
    "    with open(f\"data/history/{uid}.json\", \"r\") as f:\n",
    "        history = json.load(f)\n",
    "    return history\n",
    "\n",
    "def plot_history(history):\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.title(f\"Training history\\n\\nLR: {history['hp']['learning_rate']}, BS: {history['hp']['batch_size']}, L1: {history['hp']['l1_reg']}\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.plot(history['loss'], label=\"Loss\")\n",
    "    plt.plot(history['val_loss'], label=\"Val Loss\")\n",
    "    val_min = min(history['val_loss'])\n",
    "    plt.hlines(y=val_min, xmin=0, xmax=len(history['loss']), colors='green', linestyles='--', lw=2, label=f\"Min(Val_loss) = {val_min:.3}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "# hist = load_history(NN.uid)\n",
    "# plot_history(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2 ** 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, shuffle=True)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hp_1 = {\n",
    "    'units': (9, 27, 81, 162, 324, 648, 1296),\n",
    "    'act_fun': 'relu',\n",
    "    'learning_rate': 1E-3,\n",
    "    'batch_size': 2 ** 4,\n",
    "    'l1_reg': 1e-5,\n",
    "    'n_epochs': 200\n",
    "}\n",
    "\n",
    "hp_2 = {\n",
    "    'units': (128, 512, 1024, 4096),\n",
    "    'act_fun': 'relu',\n",
    "    'learning_rate': 1E-3,\n",
    "    'batch_size': 2 ** 5,\n",
    "    'l1_reg': 1e-4,\n",
    "    'n_epochs': 400\n",
    "}\n",
    "\n",
    "hp_3 = {\n",
    "    'units': (128, 512, 1024, 4096),\n",
    "    'act_fun': 'relu',\n",
    "    'learning_rate': 1E-3,\n",
    "    'batch_size': 2 ** 4,\n",
    "    'l1_reg': 1e-4,\n",
    "    'n_epochs': 400\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN = neural_network.NeuralNetwork(uid=\"20240506_1840\")\n",
    "hp = hp_3\n",
    "NN = neural_network.NeuralNetwork(hp)\n",
    "NN.paths['model']\n",
    "\n",
    "NN.train(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = load_history(NN.uid)\n",
    "print(hist['hp']['units'])\n",
    "last_10 = hist['val_loss'][-10:]\n",
    "print(np.mean(last_10), np.median(last_10))\n",
    "plot_history(hist)\n",
    "NN.uid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = load_history(\"20240516_1336\")\n",
    "print(hist['hp']['units'])\n",
    "last_10 = hist['val_loss'][-10:]\n",
    "print(np.mean(last_10), np.median(last_10))\n",
    "plot_history(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = load_history(\"20240516_1252\")\n",
    "print(hist['hp']['units'])\n",
    "last_10 = hist['val_loss'][-10:]\n",
    "print(np.mean(last_10), np.median(last_10))\n",
    "plot_history(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = load_history(\"20240516_1222\")\n",
    "print(hist['hp']['units'])\n",
    "last_10 = hist['val_loss'][-10:]\n",
    "print(np.mean(last_10), np.median(last_10))\n",
    "plot_history(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_train[:,0], X_train[:,1], \"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_grid = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_true, B_true = EG.sample_stack([(0, D_lim), (0, B_lim)], pred_grid)\n",
    "X_eval = np.column_stack((B_true.flatten(), D_true.flatten(), np.repeat(g, (pred_grid*pred_grid))))\n",
    "\n",
    "dB_dt, dD_dt = mm.step(B_true, D_true, g, warm_up=0)\n",
    "y_eval = np.column_stack((dB_dt.flatten(), dD_dt.flatten()))\n",
    "X_eval.shape, y_eval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = NN.model.predict(X_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_eval, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dB_dt, pred_dD_dt,  = y_pred[:,0].reshape((pred_grid,pred_grid)), y_pred[:,1].reshape((pred_grid,pred_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_true.shape, B_true.shape, pred_dD_dt.shape, pred_dB_dt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream.show(D_true, B_true, pred_dD_dt, pred_dB_dt, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surface.show(D_true, B_true, pred_dD_dt, pred_dB_dt, D_lim, B_lim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
